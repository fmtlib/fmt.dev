<pre class='metadata'>
Title: Fix the default floating-point representation in std::format
Shortname: P3505
Revision: 0
Audience: LEWG
Status: D
Group: WG21
URL:
Editor: Victor Zverovich, victor.zverovich@gmail.com
No abstract: true
Date: 2025-02-01
Markup Shorthands: markdown yes
</pre>

<p style="text-align: right">
"Is floating-point math broken?" - Cato Johnston ([Stack Overflow](
https://stackoverflow.com/q/588004/471164))
</p>

Introduction {#intro}
============

When `std::format` was proposed for standardization, floating-point formatting
was defined in terms of `std::to_chars` to simplify specification.
Unfortunately, this introduced a small but undesirable change compared to the
reference implementation in [[FMT]] and other mainstream programming languages
that have similar facilities. This paper proposes fixing this issue, bringing
the floating-point formatting on par with other languages and consistent with
the original design intent.

Problem {#problem}
=======

Since Steele and Whiteâ€™s seminal paper ([[STEELE-WHITE]]), based on their work
in the 70s, many programming languages converged on similar default
representation of floating point numbers. The properties of an algorithm
that produces such a representation are formulated in the paper as follows:

> * No information is lost; the original fraction can be recovered from the
>     output by rounding.
> * No "garbage digits" are produced.
> * The output is correctly rounded.
> * It is never necessary to propagate carries on rounding.

The second bullet point means
that the algorithm shouldn't produce more decimal digits (in the significand)
than necessary to satisfy the other requirements, most importantly the
round-trip guarantee. For example, `0.1` should be formatted as `0.1` and not
`0.10000000000000001` even though they produce the same value when read back
into an IEEE754 `double`.

The last bullet point is more of an optimization for retro computers and is less
relevant on modern systems.

[[STEELE-WHITE]] and papers that followed referred to the second criteria as
"shortness" even though it only talks about the number of decimal digits in
the significand and ignores other properties such as the exponent and the
decimal point.

Once such shortest decimal significand and the corresponding exponent are known
the formatting algorithm normally chooses between fixed and exponential
representation based on the exponent range. For example, in Python
([[PYTHON-FORMAT]]), Rust and Swift if the exponent is greater than 15 for
`double`, the number is printed in the exponential format:

```python
>>> 1234567890123456.0
1234567890123456.0
>>> 12345678901234567.0
1.2345678901234568e+16
```

15 is a reasonable choice for `double` since it's
`std::numeric_limits<double>::digits10`, or "number of decimal digits that can
be represented without change" ([[CPPREF-NUMLIMITS]]).

[[FMT]], which is modeled after Python's formatting facility, adopted a similar
representation based on the exponent range.

When `std::format` was proposed for standardization, floating-point formatting
was defined in terms of `std::to_chars` to simplify specification with the
assumption that the latter follows the industry practice for the default format
described above. It was great for explicit format specifiers such as `e` but,
as it turned out recently, it introduced an undesirable change to the default
format. This problem is that `std::to_chars` defines "shortness" in terms of the
number of characters in the output which is different from the "shortness" of
decimal significand normally used both in the literature and in the reference.

The exponent range is much easier to reason about. For example, in this modeled
`100000.0` and `120000.0` are printed in the same format:

```python
>>> 100000.0
100000.0
>>> 120000.0
120000.0
```

However, if we consider the output size the two similar numbers are now printed
completely differently:

```c++
auto s1 = std::format("{}", 100000.0);  // s1 == "1e+05"
auto s2 = std::format("{}", 120000.0);  // s2 == "120000"
```

It seems surprising and undesirable.

If the shortness of the output was indeed the main criteria then it is unclear
why the output format includes redundant `+` and leading zero in the exponent.

Even more importantly, the current representation violates the original
shortness requirement from [[STEELE-WHITE]]:

```c++
auto s = std::format("{}", 1234567890123456700000.0);
// s == "1234567890123456774144"
```

The last 5 digits, `74144`, are what Steele and White referred to as "garbage
digits" that almost no modern formatting facilities produce by default.
For example, Python avoids it by switching to the exponential format as one
would expect:

```python
>>> 12345678901234567800000.0
1.2345678901234568e+22
```

Apart from being obviously bad from the readability perspective it also has
negative performance implications. Producing "garbage digits" means that you
may no longer be able to use the optimized float-to-string algorithm such as
Dragonbox ([[DRAGONBOX]]) in some cases. It also introduces complicated logic
to handle those cases. If the fallback algorithm does multiprecision arithmetic
this may even require additional allocation(s).

The performance issue can be illustrated on the following simple benchmark:

```c++
#include <format>
#include <benchmark/benchmark.h>

double normal_input  = 12345678901234567000000.0;
double garbage_input = 1234567890123456700000.0;

void normal(benchmark::State& state) {
  for (auto s : state) {
    auto result = std::format("{}", normal_input);
    benchmark::DoNotOptimize(result);
  }
}
BENCHMARK(normal);

void garbage(benchmark::State& state) {
  for (auto s : state) {
    auto result = std::format("{}", garbage_input);
    benchmark::DoNotOptimize(result);
  }
}
BENCHMARK(garbage);

BENCHMARK_MAIN();
```

Results on macOS with Apple clang version 16.0.0 (clang-1600.0.26.6) and libc++:

```
% ./double-benchmark
Unable to determine clock rate from sysctl: hw.cpufrequency: No such file or directory
This does not affect benchmark measurements, only the metadata output.
***WARNING*** Failed to set thread affinity. Estimated CPU frequency may be incorrect.
2025-02-02T08:06:13-08:00
Running ./double-benchmark
Run on (8 X 24 MHz CPU s)
CPU Caches:
  L1 Data 64 KiB
  L1 Instruction 128 KiB
  L2 Unified 4096 KiB (x8)
Load Average: 7.61, 5.78, 5.16
-----------------------------------------------------
Benchmark           Time             CPU   Iterations
-----------------------------------------------------
normal           77.5 ns         77.5 ns      9040424
garbage          91.4 ns         91.4 ns      7675186
```

Results on GNU/Linux with gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 and
libstdc++:

```
$ ./int-benchmark
2025-02-02T17:22:25+00:00
Running ./int-benchmark
Run on (2 X 48 MHz CPU s)
CPU Caches:
  L1 Data 128 KiB (x2)
  L1 Instruction 192 KiB (x2)
  L2 Unified 12288 KiB (x2)
Load Average: 0.25, 0.10, 0.02
-----------------------------------------------------
Benchmark           Time             CPU   Iterations
-----------------------------------------------------
normal           73.1 ns         73.1 ns      9441284
garbage          90.6 ns         90.6 ns      7360351
```

Results on Windows with Microsoft (R) C/C++ Optimizing Compiler Version
19.40.33811 for ARM64 and Microsoft STL:

```
>int-benchmark.exe
2025-02-02T08:10:39-08:00
Running int-benchmark.exe
Run on (2 X 2000 MHz CPU s)
CPU Caches:
  L1 Instruction 192 KiB (x2)
  L1 Data 128 KiB (x2)
  L2 Unified 12288 KiB (x2)
-----------------------------------------------------
Benchmark           Time             CPU   Iterations
-----------------------------------------------------
normal            144 ns          143 ns      4480000
garbage           166 ns          165 ns      4072727
```

Although the output has the same size, producing "garbage digits" makes
`std::format` 15-24% slower on these inputs. If we exlude string construction
time, the difference will be even more profound. For example, profiling the
benchmark on macOS shows that the `to_chars` call itself is more than 50% (!)
slower:

```
garbage(benchmark::State&):
241.00 ms ... std::__1::to_chars_result std::__1::_Floating_to_chars[abi:ne180100]<...>(char*, char*, double, std::__1::chars_format, int)
normal(benchmark::State&):
159.00 ms ... std::__1::to_chars_result std::__1::_Floating_to_chars[abi:ne180100]<...>(char*, char*, double, std::__1::chars_format, int)
```

TODO: locale (shortness depends on the locale?!)

<!-- 
                  1  1e+00
                 12  1.2e+01
                123  1.23e+02
               1234  1.234e+03
              12345  1.2345e+04
              10000  1e+04      -> 10000
             123456  1.23456e+05
             100000  1e+05      -> 1e+05
             120000  1.2e+03    -> 120000
            1234567  1.234567e+06
           12345678  1.2345678e+07
          123456789  1.23456789e+08
         1234567891  1.234567891e+09
        12345678901  1.2345678901e+10
       123456789012  1.23456789012e+11
            ...           ...
   1234567890123456  1.234567890123456e+15
-->

Proposal {#proposal}
========

The current paper proposes fixing the default floating-point representation in
`std::format` to use exponent range, fixing the issues described above.

TODO: before / after

Optionally, the same can be done in `std::to_chars`.
TODO

Implementation and usage experience {#impl}
===================================

The current proposal is based on the existing implementation in [[FMT]] which
has been available and widely used for over 6 years. Similar logic is
implemented in Python, Java, JavaScript, Rust and Swift.

<!-- Grisu in {fmt}: https://github.com/fmtlib/fmt/issues/147#issuecomment-461118641 -->

Impact on existing code {#impact}
=======================

This may technically be a breaking change for users who rely on the exact
output that is being changed. However, the change doesn't affect ABI or round
trip guarantees. Also reliance on the exact representation of floating-point
numbers is usually discouraged so the impact of this change is likely moderate
to small. In the past we had experience with changing the output format in
[[FMT]] usage of which is currently at least an order of magnitude higher than
that of `std::format`.

Acknowledgements {#ack}
================

Thanks Junekey Jeon, the author of Dragonbox, a state-of-the-art floating-point
to string conversion algorithm, for bringing up this issue.

<pre class=biblio>
{
  "DRAGONBOX": {
    "title": "Dragonbox: A New Floating-Point Binary-to-Decimal Conversion Algorithm",
    "authors": ["Junekey Jeon"],
    "href": "https://github.com/jk-jeon/dragonbox/blob/master/other_files/Dragonbox.pdf"
  },
  "FMT": {
    "title": "The {fmt} library",
    "authors": ["Victor Zverovich"],
    "etAl": true,
    "href": "https://github.com/fmtlib/fmt"
  },
  "STEELE-WHITE": {
    "title": "How to Print Floating-Point Numbers Accurately",
    "authors": ["Guy L. Steele Jr.", "Jon L White"],
    "date": "1990",
    "publisher": "ACM"
  },
  "PYTHON-FORMAT": {
    "title": "The Python Standard Library, Format Specification Mini-Language",
    "href": "https://docs.python.org/3/library/string.html#format-specification-mini-language"
  },
  "CPPREF-NUMLIMITS": {
    "title": "C++ reference, `std::numeric_limits`",
    "href": "https://en.cppreference.com/w/cpp/types/numeric_limits"
  }
}
</pre>
