<pre class='metadata'>
Title: Fix the default floating-point representation in std::format
Shortname: P3505
Revision: 0
Audience: LEWG
Status: D
Group: WG21
URL:
Editor: Victor Zverovich, victor.zverovich@gmail.com
No abstract: true
Date: 2025-02-01
Markup Shorthands: markdown yes
</pre>

<p style="text-align: right">
"Is floating-point math broken?" - Cato Johnston ([Stack Overflow](
https://stackoverflow.com/q/588004/471164))
</p>

Introduction {#intro}
============

When `std::format` was proposed for standardization, floating-point formatting
was defined in terms of `std::to_chars` to simplify specification.
Unfortunately, this introduced a small but undesirable change compared to the
reference implementation in [[FMT]], resulting in surprising behavior to users,
performance regression and an inconsistency with other mainstream programming
languages that have similar facilities. This paper proposes fixing this issue,
bringing the floating-point formatting on par with other languages and
consistent with the original design intent.

Problem {#problem}
=======

Since Steele and White’s seminal paper ([[STEELE-WHITE]]), based on their work
in the 70s, many programming languages converged on similar default
representation of floating point numbers. The properties of an algorithm
that produces such a representation are formulated in the paper as follows:

> * No information is lost; the original fraction can be recovered from the
>     output by rounding.
> * No "garbage digits" are produced.
> * The output is correctly rounded.
> * It is never necessary to propagate carries on rounding.

The second bullet point means
that the algorithm shouldn't produce more decimal digits (in the significand)
than necessary to satisfy the other requirements, most importantly the
round-trip guarantee. For example, `0.1` should be formatted as `0.1` and not
`0.10000000000000001` even though they produce the same value when read back
into an IEEE754 `double`.

The last bullet point is more of an optimization for retro computers and is less
relevant on modern systems.

[[STEELE-WHITE]] and papers that followed referred to the second criteria as
"shortness" even though it only talks about the number of decimal digits in
the significand and ignores other properties such as the exponent and the
decimal point.

Once such shortest decimal significand and the corresponding exponent are known
the formatting algorithm normally chooses between fixed and exponential
representation based on the value range. For example, in Python
([[PYTHON-FORMAT]]) and Rust (the ``?`` format which gives the "shortest"
representation for FP numbers) if the decimal exponent is greater or equal to 16
for `double`, the number is printed in the exponential format:

<!--
Python:
>>> 9999999999999998.0
9999999999999998.0
>>> 10000000000000000.0
1e+16

Rust - same as Python: https://rust.godbolt.org/z/b7sYd7jc5
-->

```python
>>> 1234567890123456.0
1234567890123456.0
>>> 12345678901234567.0
1.2345678901234568e+16
```

16 is a reasonable threshold because IEEE 754 double precision can represent
most 16-digit decimal values with high fidelity, and it balances human
readability with precision retention when switching between fixed and
exponential notation.

[[FMT]], which is modeled after Python's formatting facility, adopted a similar
representation based on the exponent range.

Swift has similar logic, switching to exponential notation for numbers greater
or equal to 2<sup>53</sup> (9007199254740992). This is also a reasonable choice
although a threshold that is not a power of 10 might be less intuitive to some
users.

Similarly, languages normally switch from fixed to exponential notation when
the absolute value is smaller than some small decimal power of 10, usually
10<sup>-3</sup> ([[JAVA-DOUBLE]]) or 10<sup>-4</sup> (Python, Rust, Swift).

<!--
Swift logic to switch to exponential:
https://github.com/swiftlang/swift/blob/main/stdlib/public/runtime/SwiftDtoa.cpp#L1261
https://swift.godbolt.org/z/68nneoTW7
-->

When `std::format` was proposed for standardization, floating-point formatting
was defined in terms of `std::to_chars` to simplify specification with the
assumption that the latter follows the industry practice for the default format
described above. It was great for explicit format specifiers such as `e` but,
as it turned out recently, it introduced an undesirable change to the default
format. This problem is that `std::to_chars` defines "shortness" in terms of the
number of characters in the output which is different from the "shortness" of
decimal significand normally used both in the literature and in the reference.

The exponent range is much easier to reason about. For example, in this model
`100000.0` and `120000.0` are printed in the same format:

```python
>>> 100000.0
100000.0
>>> 120000.0
120000.0
```

However, if we consider the output size the two similar numbers are now printed
completely differently:

```c++
auto s1 = std::format("{}", 100000.0);  // s1 == "1e+05"
auto s2 = std::format("{}", 120000.0);  // s2 == "120000"
```

It seems surprising and undesirable.

If the shortness of the output was indeed the main criteria then it is unclear
why the output format includes redundant `+` and leading zero in the exponent.

Even more importantly, the current representation violates the original
shortness requirement from [[STEELE-WHITE]]:

```c++
auto s = std::format("{}", 1234567890123456700000.0);
// s == "1234567890123456774144"
```

The last 5 digits, `74144`, are what Steele and White referred to as "garbage
digits" that almost no modern formatting facilities produce by default.
For example, Python avoids it by switching to the exponential format as one
would expect:

```python
>>> 12345678901234567800000.0
1.2345678901234568e+22
```

Apart from being obviously bad from the readability perspective it also has
negative performance implications. Producing "garbage digits" means that you
may no longer be able to use the optimized float-to-string algorithm such as
Dragonbox ([[DRAGONBOX]]) in some cases. It also introduces complicated logic
to handle those cases. If the fallback algorithm does multiprecision arithmetic
this may even require additional allocation(s).

The performance issue can be illustrated on the following simple benchmark:

```c++
#include <format>
#include <benchmark/benchmark.h>

double normal_input  = 12345678901234567000000.0;
double garbage_input = 1234567890123456700000.0;

void normal(benchmark::State& state) {
  for (auto s : state) {
    auto result = std::format("{}", normal_input);
    benchmark::DoNotOptimize(result);
  }
}
BENCHMARK(normal);

void garbage(benchmark::State& state) {
  for (auto s : state) {
    auto result = std::format("{}", garbage_input);
    benchmark::DoNotOptimize(result);
  }
}
BENCHMARK(garbage);

BENCHMARK_MAIN();
```

Results on macOS with Apple clang version 16.0.0 (clang-1600.0.26.6) and libc++:

```
% ./double-benchmark
Unable to determine clock rate from sysctl: hw.cpufrequency: No such file or directory
This does not affect benchmark measurements, only the metadata output.
***WARNING*** Failed to set thread affinity. Estimated CPU frequency may be incorrect.
2025-02-02T08:06:13-08:00
Running ./double-benchmark
Run on (8 X 24 MHz CPU s)
CPU Caches:
  L1 Data 64 KiB
  L1 Instruction 128 KiB
  L2 Unified 4096 KiB (x8)
Load Average: 7.61, 5.78, 5.16
-----------------------------------------------------
Benchmark           Time             CPU   Iterations
-----------------------------------------------------
normal           77.5 ns         77.5 ns      9040424
garbage          91.4 ns         91.4 ns      7675186
```

Results on GNU/Linux with gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 and
libstdc++:

```
$ ./int-benchmark
2025-02-02T17:22:25+00:00
Running ./int-benchmark
Run on (2 X 48 MHz CPU s)
CPU Caches:
  L1 Data 128 KiB (x2)
  L1 Instruction 192 KiB (x2)
  L2 Unified 12288 KiB (x2)
Load Average: 0.25, 0.10, 0.02
-----------------------------------------------------
Benchmark           Time             CPU   Iterations
-----------------------------------------------------
normal           73.1 ns         73.1 ns      9441284
garbage          90.6 ns         90.6 ns      7360351
```

Results on Windows with Microsoft (R) C/C++ Optimizing Compiler Version
19.40.33811 for ARM64 and Microsoft STL:

```
>int-benchmark.exe
2025-02-02T08:10:39-08:00
Running int-benchmark.exe
Run on (2 X 2000 MHz CPU s)
CPU Caches:
  L1 Instruction 192 KiB (x2)
  L1 Data 128 KiB (x2)
  L2 Unified 12288 KiB (x2)
-----------------------------------------------------
Benchmark           Time             CPU   Iterations
-----------------------------------------------------
normal            144 ns          143 ns      4480000
garbage           166 ns          165 ns      4072727
```

Although the output has the same size, producing "garbage digits" makes
`std::format` 15-24% slower on these inputs. If we exlude string construction
time, the difference will be even more profound. For example, profiling the
benchmark on macOS shows that the `to_chars` call itself is more than 50% (!)
slower:

```
garbage(benchmark::State&):
241.00 ms ... std::__1::to_chars_result std::__1::_Floating_to_chars[abi:ne180100]<...>(char*, char*, double, std::__1::chars_format, int)
normal(benchmark::State&):
159.00 ms ... std::__1::to_chars_result std::__1::_Floating_to_chars[abi:ne180100]<...>(char*, char*, double, std::__1::chars_format, int)
```

Locale makes the situation even more confusing to users. Consider the following
example:

```c++
std::locale::global(std::locale("en_US.UTF-8"));
auto s = std::format("{:L}", 1200000.0);  // s == "1,200,000"
```

Here `s` is `"1,200,000"` even though `"1.2e+06"` would be shorter.

<!-- https://www.godbolt.org/z/nnWG8rxzq -->

<!-- 
                  1  1e+00
                 12  1.2e+01
                123  1.23e+02
               1234  1.234e+03
              12345  1.2345e+04
              10000  1e+04      -> 10000
             123456  1.23456e+05
             100000  1e+05      -> 1e+05
             120000  1.2e+03    -> 120000
            1234567  1.234567e+06
           12345678  1.2345678e+07
          123456789  1.23456789e+08
         1234567891  1.234567891e+09
        12345678901  1.2345678901e+10
       123456789012  1.23456789012e+11
            ...           ...
   1234567890123456  1.234567890123456e+15
-->

Proposal {#proposal}
========

The current paper proposes fixing the default floating-point representation in
`std::format` to use exponent range, fixing the issues described above.

Consistent, easy to reason about output format:

<table>
<tr>
  <th>Code
  <th>Before
  <th>After
</tr>
<tr>
<td>
```
std::format("{}", 100000.0)
```
<td>
```
"1e+05"
```
<td>
```
"100000"
```
</tr>
<tr>
<td>
```
std::format("{}", 120000.0)
```
<td>
```
"120000"
```
<td>
```
"120000"
```
</tr>
</table>

No "garbage digits":

<table>
<tr>
  <th>Code
  <th>Before
  <th>After
</tr>
<tr>
<td>
```
std::format("{}", 1234567890123456700000.0)
```
<td>
```
"1234567890123456774144"
```
<td>
```
"1.2345678901234568e+22"
```
</tr>
</table>

Consistent localized output (assuming <code highlight="text">en_US.UTF-8</code>
locale):

<table>
<tr>
  <th>Code
  <th>Before
  <th>After
</tr>
<tr>
<td>
```
std::format("{:L}", 1000000.0)
```
<td>
```
"1e+06"
```
<td>
```
"1,000,000"
```
</tr>
<tr>
<td>
```
std::format("{:L}", 1200000.0)
```
<td>
```
"1,200,000"
```
<td>
```
"1,200,000"
```
</tr>
</table>

<!-- TODO: discuss doing it on to_chars vs format level -->

Wording {#wording}
=======

Modify [[format.string.std](https://eel.is/c++draft/format.string.std)]:

Table 105 — Meaning of type options for floating-point types
[[tab:format.type.float](https://eel.is/c++draft/format.string.std)]

<table>
<tr>
  <td>**Type**
  <td>**Meaning**
</tr>
<tr>
  <td>`a`
  <td>
    If *precision* is specified, equivalent to
    ```
      to_chars(first, last, value, chars_format::hex, precision)
    ```
    where `precision` is the specified formatting precision; equivalent to
    ```
      to_chars(first, last, value, chars_format::hex)
    ```
    otherwise.
</tr>
<tr>
  <td>...
  <td>...
</tr>
<tr>
  <td>`G`
  <td>The same as `g`, except that it uses `E` to indicate exponent.
</tr>
<tr>
  <td>none
  <td>
    <ins>Let <code>fmt</code> be `chars_format::fixed` if <code>value</code>
    is in the range [10<sup>-4</sup>, 10<sup><i>n</i></sup>), where <i>n</i> is
    2<sup><code>std::numeric_limits&lt;decltype(value)&gt;::digits</code></sup>
    rounded to the nearest power of 10, `chars_format::scientific` otherwise.
    </ins>
    
    If *precision* is specified, equivalent to
    ```
      to_chars(first, last, value, chars_format::general, precision)
    ```
    where `precision` is the specified formatting precision; equivalent to
    <pre>
      to_chars(first, last, value<ins>, fmt</ins>)
    </pre>
    otherwise.
</tr>
</table>

Implementation and usage experience {#impl}
===================================

The current proposal is based on the existing implementation in [[FMT]] which
has been available and widely used for over 6 years. Similar logic based on the
value range rather than output size is implemented in Python, Java, JavaScript,
Rust and Swift.

<!-- Grisu in {fmt}: https://github.com/fmtlib/fmt/issues/147#issuecomment-461118641 -->

Impact on existing code {#impact}
=======================

This is technically a breaking change for users who rely on the exact
output that is being changed. However, the change doesn't affect ABI or round
trip guarantees. Also reliance on the exact representation of floating-point
numbers is usually discouraged so the impact of this change is likely moderate.
In the past we had experience with changing the output format in [[FMT]], usage
of which is currently at least an order of magnitude higher than that of
`std::format`.

Acknowledgements {#ack}
================

Thanks Junekey Jeon, the author of Dragonbox, a state-of-the-art floating-point
to string conversion algorithm, for bringing up this issue.

<pre class=biblio>
{
  "DRAGONBOX": {
    "title": "Dragonbox: A New Floating-Point Binary-to-Decimal Conversion Algorithm",
    "authors": ["Junekey Jeon"],
    "href": "https://github.com/jk-jeon/dragonbox/blob/master/other_files/Dragonbox.pdf"
  },
  "FMT": {
    "title": "The {fmt} library",
    "authors": ["Victor Zverovich"],
    "etAl": true,
    "href": "https://github.com/fmtlib/fmt"
  },
  "STEELE-WHITE": {
    "title": "How to Print Floating-Point Numbers Accurately",
    "authors": ["Guy L. Steele Jr.", "Jon L White"],
    "date": "1990",
    "publisher": "ACM"
  },
  "PYTHON-FORMAT": {
    "title": "The Python Standard Library, Format Specification Mini-Language",
    "href": "https://docs.python.org/3/library/string.html#format-specification-mini-language"
  },
  "JAVA-DOUBLE": {
    "title": "Java™ Platform, Standard Edition 8 API Specification, Class Double, toString",
    "href": "https://docs.oracle.com/javase/8/docs/api/java/lang/Double.html#toString-double-"
  }
}
</pre>
